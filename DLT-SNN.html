<!DOCTYPE HTML>
<html>
	<head>
		<title> ThetaLIF-SNN for Efficient Classification </title>
		<link rel="icon" href="images/pic5.jpg">
		
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	 <style>
    .dynamic-bg {
      background-image: url("images/bg.jpg");
      background-position: center;
      background-repeat: no-repeat;
      background-size: cover;
      background-attachment: fixed;
      height: 100vh;
    }
	.post p {
  text-align: justify !important; /* Changed from left to justify for paper look */
}
/* Styling for mathematical variables */
.math-var {
    font-style: italic;
    font-family: serif;
}
/* Styling for figure captions to stand out (centered block) */
.figure-caption {
    font-size: 0.9em;
    text-align: left; /* Ensure text starts at the left of the caption block */
    margin: 15px auto;
    border-top: 1px solid #ccc;
    padding-top: 10px;
    width: 90%; /* Give it a defined width to align its internal text */
}
.figure-caption strong {
    display: block; /* Ensure the figure title is on its own line */
    text-align: center;
    margin-bottom: 5px;
}
/* Styling for layer transition arrows */
.layer-transition {
    font-size: 1.1em;
    font-weight: bold;
    color: #444;
    margin: 0 5px;
}
.layer-dims {
    font-size: 1.1em;
    font-family: monospace;
    font-weight: bold;
    padding: 0 5px;
}

</style>
</head>

<body class="dynamic-bg">

				<header id="header">
						<a class="logo"> Spiking Neural Networks Research </a>
					</header>

				<nav id="nav">
						<ul class="links">
</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/saba-madadi-8a7374256/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="https://github.com/sabamadadi" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<div id="main">

						<section class="post" style="text-align: left !important; direction: ltr; margin: 0 auto;">
  <header class="major" style="text-align: left !important;">
<h3>A Dynamic Threshold Leaky Integrate-and-Fire Spiking Neural Network (ThetaLIF-SNN) for Efficient Image Classification</h3>
  </header>

  <h3>Abstract</h3>
  <p>
  	Spiking Neural Networks (SNNs) are gaining prominence for their energy efficiency, but their performance is often hampered by the static nature of their neuron parameters, such as the firing threshold (<span class="math-var">&Theta;</span>). This paper introduces the <strong>ThetaLIF Node</strong>, a novel Leaky Integrate-and-Fire (LIF) neuron variant featuring a <strong>state-dependent, dynamic threshold</strong>. The threshold is predicted at each time step by a feed-forward network based on the neuron's previous state variables, enabling adaptation to input dynamics. We evaluate a four-layer ThetaLIF-SNN on a 10-class image classification task, demonstrating the feasibility of integrating adaptive parameters within a standard SNN architecture using backpropagation through time. Initial results show promising performance improvement over epochs, suggesting the dynamic threshold mechanism enhances the network's learning capability and temporal representation capacity.
  </p>

  <h3>1. Introduction</h3>
  <p>
  	The quest for more biologically plausible and energy-efficient neural networks has driven the recent resurgence of Spiking Neural Networks (SNNs). Unlike conventional deep learning models, SNNs rely on sparse, discrete spike events, which promise significant power savings on neuromorphic hardware. However, training deep SNNs remains challenging, often requiring surrogate gradient methods and suffering from performance limitations due to the fixed nature of key neuron parameters like the membrane time constant (<span class="math-var">&tau;</span>) and the firing threshold (<span class="math-var">&Theta;</span>). This fixed-parameter constraint limits the neuron's ability to adapt its firing behavior dynamically to the input sequence over time. To address this, we propose the <strong>ThetaLIF Node</strong>, a custom LIF neuron with a dynamic, learnable threshold, and demonstrate its integration into a multi-layer SNN architecture.
  </p>

  <h3>2. Model Architecture and Methodology</h3>
  <h4>2.1 Custom LIF Neuron Implementation (CLIFNode)</h4>
  <p>
  	The foundation of our network is the custom <code>CLIFNode</code>, which inherits from a standard Leaky Integrate-and-Fire (LIF) model. The implementation is optimized for parallel computation on the GPU (<code>device = 'cuda'</code>) and supports efficient single-step and multi-step forward passes via a CuPy backend, crucial for accelerating the training of time-based SNNs. This customization allows for specialized backpropagation kernels and efficient memory management during training.
  </p>

  <h4>2.2 Dynamic Threshold Neuron (ThetaLIFNode)</h4>
  <p>
  	The core innovation is the <code>ThetaLIFNode</code>, which modifies the standard LIF model by making the firing threshold (<span class="math-var">&Theta;</span>) dynamic. Instead of a fixed value, the threshold for the next time step is determined by a small Multi-Layer Perceptron (MLP), defined as <code>self.theta_linear</code>. This MLP takes a concatenated vector of the neuron's previous state variables as input:
  </p>
  <ul>
  	<li><span class="math-var">S<sub>t-1</sub></span>: The spike output from the previous time step.</li>
  	<li><span class="math-var">&Theta;<sub>t-1</sub></span>: The firing threshold value from the previous time step.</li>
  	<li><span class="math-var">&mu;(V<sub>t-1</sub>)</span>: The mean-subtracted membrane potential from the previous time step.</li>
  	<li><span class="math-var">&mu;(&Theta;<sub>t-1</sub>)</span>: The mean-subtracted previous threshold.</li>
  </ul>
  <p>
  	<span class="math-var">&Theta;<sub>t</sub></span> calculation is formalized as: 
	<div style="text-align: center; font-size: 1.1em; margin: 15px 0;">
		<strong>&Theta;<sub>t</sub> = MLP([<span class="math-var">S<sub>t-1</sub></span>, <span class="math-var">&Theta;<sub>t-1</sub></span>, <span class="math-var">&mu;</span>(<span class="math-var">V<sub>t-1</sub></span>), <span class="math-var">&mu;</span>(<span class="math-var">&Theta;<sub>t-1</sub></span>)])</strong>
	</div>
  	This recurrent structure allows the neuron's sensitivity to inputs to adapt based on its short-term history, a key feature for modeling complex temporal dependencies in sequential data.
  </p>


  <h4>2.3 SNN Architecture and Training</h4>
  <p>
  	The overall network, named <code>SNN_MNIST</code>, is a four-layer fully connected SNN:
  </p>
  <ul>
  	<li><strong>Input/Encoder:</strong> Raw input is encoded into spike trains using a <strong>Poisson Encoder</strong>. The input layer size is 3072, suggesting a flattened <span class="math-var">32 &times; 32 &times; 3</span> image input (e.g., a variant of MNIST or another small image dataset).</li>
  	<li><strong>Network Topology:</strong> The network topology is defined by the following layer-wise transitions: 
        <div style="text-align: center; margin: 10px 0;">
            <span class="layer-dims">3072</span> <span class="layer-transition"> &rarr; </span> <span class="layer-dims">256</span> <span class="layer-transition"> &rarr; </span> <span class="layer-dims">64</span> <span class="layer-transition"> &rarr; </span> <span class="layer-dims">32</span> <span class="layer-transition"> &rarr; </span> <span class="layer-dims">10</span>
        </div>
        Each transition consists of a <strong>Linear layer</strong> followed by a <strong>ThetaLIFNode</strong> for spiking non-linearity. The output layer has 10 neurons for classification.
    </li>
  	<li><strong>Temporal Simulation:</strong> The network runs for <span class="math-var">T=20</span> time steps. The final output is determined by the average firing rate (<code>out_fr / self.T</code>) of the output layer.</li>
  	<li><strong>Optimization:</strong> The model is trained using the <strong>Adam optimizer</strong> with a learning rate of <span class="layer-dims">0.0001</span>. Mixed-precision training (<code>amp.GradScaler</code>, <code>amp.autocast</code>) is employed for faster convergence and reduced memory footprint.</li>
  </ul>


  <h3>3. Experimental Results</h3>
  <p>
  	The model was tested on a 10-class image classification task. The performance metrics were monitored over four epochs (and into the fifth), tracking training and test accuracy:
  </p>
  <ul>
  	<li><strong>Epoch 0:</strong> Train Acc: 28.83%, Test Acc: 35.48%.</li>
  	<li><strong>Epoch 1:</strong> Train Acc: 36.70%, Test Acc: 38.38%.</li>
  	<li><strong>Epoch 2:</strong> Train Acc: 37.31%, Test Acc: 39.46%.</li>
  	<li><strong>Epoch 3:</strong> Train Acc: 41.28%, Test Acc: 41.19%.</li>
  	<li><strong>Epoch 4:</strong> Train Acc: 42.68%, Test Acc: 41.94%.</li>
  	<li><strong>Epoch 5 (Partial):</strong> Checkpoint accuracy reached 45.66%.</li>
  </ul>
  <p>
  	The consistent increase in test accuracy from 35.48% to over 45% demonstrates that the network effectively learns the task over training time. The relatively low baseline accuracy suggests the dataset is challenging (possibly CIFAR-10) or that the network requires further optimization, but the learning curve indicates the dynamic threshold is functional and capable of contributing to performance gains.
  </p>


    <p>
        Further analysis is required to fully characterize the dynamic behavior of the <code>ThetaLIF</code> node. Observing the change in neuron state over time reveals the adaptive nature of the threshold.
    </p>

							<div class="figure">
  <div class="figure-caption">
    <strong>Figure: Membrane Potential (V) & Threshold (Θ) + Spike Output (S)</strong> 
    A time-series plot showing the membrane potential (<span class="math-var">V</span>), 
    the spike output (<span class="math-var">S</span>), and the 
    dynamically changing threshold (<span class="math-var">&Theta;</span>) 
    for a single example neuron in one of the hidden layers 
    (e.g., the last layer) over the <span class="math-var">T=20</span> time steps. 
    This figure is crucial to visually demonstrate the core mechanism of the 
    <code>ThetaLIF</code> node.
  </div>

  <!-- دو نمودار بغل هم -->
  <div class="row">
    <div class="col">
      <img src="output2.png" alt="Spike Output (S)" class="plot">
      <p class="caption">Spike Output (S)</p>
    </div>
    <div class="col">
      <img src="output.png" alt="Membrane Potential and Threshold" class="plot">
      <p class="caption">Membrane Potential (V) & Threshold (Θ)</p>
    </div>
  </div>
</div>

<style>
  .row {
    display: flex;
    justify-content: center;
    gap: 20px;       /* فاصله بین نمودارها */
    margin-top: 10px;
  }
  .col {
    flex: 1;
    text-align: center;
  }
  .plot {
    width: 90%;      /* سایز کوچیک‌تر */
    max-width: 350px;
    border: 1px solid #ccc;
    border-radius: 6px;
  }
  .caption {
    margin-top: 4px;
    font-size: 13px;
  }
  .figure-caption {
    margin-bottom: 15px;
    font-size: 14px;
  }
</style>

  <h3>4. Conclusion</h3>
  <p>
  	We successfully implemented and tested a Spiking Neural Network featuring a novel <strong>Dynamic Threshold LIF (ThetaLIF)</strong> neuron. The <code>ThetaLIF</code> node allows the firing threshold to adapt based on the neuron's previous spike output, membrane potential, and threshold value. Initial experimental results on a 10-class image classification task confirm the network's ability to learn and improve performance over successive epochs. Future work will focus on comparative analysis against fixed-threshold LIF models and exploring more sophisticated linear/non-linear mappings for the threshold prediction to further enhance the SNN's efficiency and accuracy.
  </p>

								
</section>


					</div>

				<footer id="footer">
						<section class="split contact">
							<section>
								<h3>Email </h3>
								<p>
							            <a href="mailto:sa.madadi@mail.sbu.ac.ir">mailto:sa.madadi@mail.sbu.ac.ir</a>
							        </p>  
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://www.linkedin.com/in/saba-madadi-8a7374256/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="https://github.com/sabamadadi" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>
				<div id="">
						<ul><li></li><li><a href=""></a></li></ul>
					</div>

			</div>

		<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
